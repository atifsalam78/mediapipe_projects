{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9fd56f4d-eafa-4528-b4ed-62372a6914e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "57a58e76-4f94-4869-ac90-9efe02d51e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('data') \n",
    "\n",
    "# Actions that we try to detect\n",
    "# actions = np.array(['hello', 'thanks', 'iloveyou'])\n",
    "actions = np.array(['yes', 'no'])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "for action in actions:    \n",
    "    for sequence in range(no_sequences):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4b7bcb15-bf9a-4efe-be08-81844f96da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB) # Color conversion from BGR to RGB\n",
    "    image.flags.writeable = False                  # Image is no longe writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable\n",
    "    image = cv.cvtColor(image, cv.COLOR_RGB2BGR) # Color converson from RGB to BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e6cc7570-93dd-4ab5-807b-6a1c3008b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw Face connections - if we want just outlines of the face\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             )\n",
    "\n",
    "                            # OR if we want mesh\n",
    "    # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "    #                          mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "    #                          mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "    #                          )\n",
    "    # # Draw Pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=2),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    # Draw Left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=2),\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,121), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    # Draw Right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d4710a7f-9990-4c29-859b-ab2e82b23b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a17dd9f0-ba55-4ed0-9637-cad8107d3b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic   # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "# Set webcam width and height\n",
    "cap.set(3,1280)\n",
    "\n",
    "# set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    #NEW LOOP\n",
    "    # Loop through action\n",
    "    for action in actions:\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(no_sequences):\n",
    "            # Loop through video length aka sequences length\n",
    "            for frame_num in range(sequence_length):\n",
    "                \n",
    "                # Read Frame\n",
    "                ret, frame = cap.read()\n",
    "                frame = cv.flip(frame, 1)\n",
    "                \n",
    "                # Make detection\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                \n",
    "                # Draw Landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # New apply wait logic\n",
    "                if frame_num == 0:\n",
    "                    cv.putText(image, \"Starting Collection\", (120,200),\n",
    "                               cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv.LINE_AA)\n",
    "                    cv.putText(image, \"Collecting frames for {} video number {}\".format(action, sequence), (15,12),\n",
    "                               cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv.LINE_AA)\n",
    "                    \n",
    "                    # Show to Screen\n",
    "                    cv.imshow(\"Opencv Feed\", image)\n",
    "                    cv.waitKey(300)\n",
    "                    \n",
    "                else:\n",
    "                    cv.putText(image, \"Collecting frames for {} video number {}\".format(action, sequence), (15,12),\n",
    "                               cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv.LINE_AA)\n",
    "                    cv.imshow(\"Opencv Feed\", image)\n",
    "\n",
    "                # New Keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "                \n",
    "                # Brake gracefully\n",
    "                if cv.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "                    break\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97de3d9f-deb5-474f-997b-b7d1a7e1b15b",
   "metadata": {},
   "source": [
    "# Preporcess data and create labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e3816c46-60dc-4305-9989-4a7023c374b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "573aab3e-0468-4e19-8b03-1986eaba8481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 0, 'thanks': 1, 'iloveyou': 2}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d106d3e0-a6ea-4d51-bfa4-6ed23db96c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ab3de3e1-e6dd-42f1-833e-d72d485ff637",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7b6d8ef5-60f0-4adf-8436-d74fd927c787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 30, 1662)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e1a7f828-2af6-407c-8940-0ddf91e58bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "68470965-1a7d-4c56-b501-3022ee7173bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d2015b1b-8042-4824-aef7-a46e239ddef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "20a331ef-2597-4600-9037-e86504dd3d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 30, 1662)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "082144a1-0b72-4321-a106-213bf83318c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 30, 1662)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eb5f09c-1215-4937-8067-06a5fb5addbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap.release()\n",
    "# cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b5f035-17a8-41a6-bbf6-8816aea55edf",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc208de-bb25-4b82-9702-29dab04a57f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f599aa60-a3b1-49b5-8d37-e22cbac9c7ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imgaug'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimgaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmenters\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01miaa\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imgaug'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import imgaug.augmenters as iaa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "12b66a50-89e9-4871-8cc1-e180db25e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(restore_best_weights=True,patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eda8b32a-3d3b-42ee-837e-0b7fa3c1c741",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_height' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m----> 2\u001b[0m         TimeDistributed(Conv2D(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m3\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(sequence_length, image_height, image_width, \u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m      3\u001b[0m                                padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m      4\u001b[0m         TimeDistributed(BatchNormalization()),\n\u001b[0;32m      5\u001b[0m         TimeDistributed(MaxPooling2D()),\n\u001b[0;32m      6\u001b[0m         TimeDistributed(Dropout(\u001b[38;5;241m0.3\u001b[39m)),\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m         TimeDistributed(Conv2D(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m3\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m      9\u001b[0m         TimeDistributed(BatchNormalization()),\n\u001b[0;32m     10\u001b[0m         TimeDistributed(MaxPooling2D()),\n\u001b[0;32m     11\u001b[0m         TimeDistributed(Dropout(\u001b[38;5;241m0.3\u001b[39m)),\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m         TimeDistributed(Conv2D(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m3\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m     14\u001b[0m         TimeDistributed(BatchNormalization()),\n\u001b[0;32m     15\u001b[0m         TimeDistributed(MaxPooling2D()),\n\u001b[0;32m     16\u001b[0m         TimeDistributed(Dropout(\u001b[38;5;241m0.3\u001b[39m)),\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m         TimeDistributed(Conv2D(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m3\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m     19\u001b[0m         TimeDistributed(BatchNormalization()),\n\u001b[0;32m     20\u001b[0m         TimeDistributed(MaxPooling2D()),\n\u001b[0;32m     21\u001b[0m         TimeDistributed(Dropout(\u001b[38;5;241m0.3\u001b[39m)),\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m         TimeDistributed(Flatten()),\n\u001b[0;32m     24\u001b[0m         LSTM(\u001b[38;5;241m32\u001b[39m),\n\u001b[0;32m     25\u001b[0m         Dense(\u001b[38;5;241m4\u001b[39m),\n\u001b[0;32m     26\u001b[0m     ])\n\u001b[0;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     29\u001b[0m         loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     30\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(),\n\u001b[0;32m     31\u001b[0m         metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     32\u001b[0m     )\n\u001b[0;32m     34\u001b[0m model_train_hist \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     35\u001b[0m         X_train, y_train,\n\u001b[0;32m     36\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m[early_stopping],\n\u001b[0;32m     41\u001b[0m     )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_height' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    all_data_dir = 'data'\n",
    "    image_height, image_width = 120, 160\n",
    "    sequence_length = 8\n",
    "    X, y = [], []\n",
    "\n",
    "    image_seq_augmenter = iaa.Sequential([\n",
    "        iaa.Fliplr(0),\n",
    "        iaa.Crop(percent=(0, 0.1)),\n",
    "        iaa.LinearContrast((0.75, 1.5)),\n",
    "        iaa.GaussianBlur(sigma=(0.0, 1.0)),\n",
    "        iaa.Multiply((0.8, 1.2), per_channel=0.2)\n",
    "    ])\n",
    "\n",
    "    for idx, class_name in enumerate(os.listdir(all_data_dir)):\n",
    "        for image_seq_name in os.listdir(os.path.join(all_data_dir, class_name)):\n",
    "            image_seq = []\n",
    "            for frame_name in os.listdir(os.path.join(all_data_dir, class_name, image_seq_name)):\n",
    "                frame = cv2.imread(os.path.join(all_data_dir, class_name, image_seq_name, frame_name))\n",
    "                frame = cv2.resize(frame, (image_height, image_width))\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                image_seq.append(frame)\n",
    "            image_seq_aug = image_seq_augmenter(images=image_seq)\n",
    "            X.extend([image_seq, image_seq_aug])\n",
    "            y.extend([idx for i in range(2)])\n",
    "\n",
    "    X = (np.array(X) / 255.0).astype('float32') # (n_samples, n_frames, height, width, channels)\n",
    "    y = np.array(y)                             # (n_samples)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=1)\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(restore_best_weights=True,\n",
    "                                                      patience=10)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        TimeDistributed(Conv2D(16, 3, activation='relu', input_shape=(sequence_length, image_height, image_width, 3),\n",
    "                   padding='same')),\n",
    "        TimeDistributed(BatchNormalization()),\n",
    "        TimeDistributed(MaxPooling2D()),\n",
    "        TimeDistributed(Dropout(0.3)),\n",
    "\n",
    "        TimeDistributed(Conv2D(32, 3, activation='relu', padding='same')),\n",
    "        TimeDistributed(BatchNormalization()),\n",
    "        TimeDistributed(MaxPooling2D()),\n",
    "        TimeDistributed(Dropout(0.3)),\n",
    "\n",
    "        TimeDistributed(Conv2D(64, 3, activation='relu', padding='same')),\n",
    "        TimeDistributed(BatchNormalization()),\n",
    "        TimeDistributed(MaxPooling2D()),\n",
    "        TimeDistributed(Dropout(0.3)),\n",
    "\n",
    "        TimeDistributed(Conv2D(64, 3, activation='relu', padding='same')),\n",
    "        TimeDistributed(BatchNormalization()),\n",
    "        TimeDistributed(MaxPooling2D()),\n",
    "        TimeDistributed(Dropout(0.3)),\n",
    "\n",
    "        TimeDistributed(Flatten()),\n",
    "        LSTM(32),\n",
    "        Dense(4),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    model_train_hist = model.fit(\n",
    "        X_train, y_train,\n",
    "        shuffle=True,\n",
    "        batch_size=4,\n",
    "        epochs=70,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "    )\n",
    "\n",
    "    model_eval_loss, model_eval_acc = model.evaluate(X_test, y_test)\n",
    "    date_time_format = '%Y_%m_%d__%H_%M_%S'\n",
    "    current_date_time_dt = dt.datetime.now()\n",
    "    current_date_time_str = dt.datetime.strftime(current_date_time_dt, date_time_format)\n",
    "\n",
    "    model_name = f'model__date_time_{current_date_time_str}__loss_{model_eval_loss}__acc_{model_eval_acc}__hand.h5'\n",
    "    model.save(model_name)\n",
    "\n",
    "    df_train_hist = pd.DataFrame(model_train_hist.history)\n",
    "    df_train_hist.loc[:, ['loss', 'val_loss']].plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129f55cc-87a5-49e7-b7f7-30133a096bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
